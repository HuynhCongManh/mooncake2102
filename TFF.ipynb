{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mooncake2102/mooncake2102/blob/main/TFF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubPayUtQR-h6",
        "outputId": "f26b67ef-b611-4455-a79c-2439dae2df9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 821 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 28.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 60.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 126 kB 57.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 887 kB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 908 kB/s \n",
            "\u001b[K     |████████████████████████████████| 65.2 MB 106 kB/s \n",
            "\u001b[K     |████████████████████████████████| 251 kB 63.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 238 kB 82.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 553.8 MB 20 kB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 553.8 MB 18 kB/s \n",
            "\u001b[K     |████████████████████████████████| 553.2 MB 27 kB/s \n",
            "\u001b[K     |████████████████████████████████| 552.9 MB 25 kB/s \n",
            "\u001b[K     |████████████████████████████████| 551.1 MB 19 kB/s \n",
            "\u001b[K     |████████████████████████████████| 551.1 MB 16 kB/s \n",
            "\u001b[K     |████████████████████████████████| 551.1 MB 21 kB/s \n",
            "\u001b[K     |████████████████████████████████| 551.1 MB 12 kB/s \n",
            "\u001b[K     |████████████████████████████████| 551.1 MB 9.8 kB/s \n",
            "\u001b[K     |████████████████████████████████| 551.0 MB 26 kB/s \n",
            "\u001b[K     |████████████████████████████████| 551.0 MB 21 kB/s \n",
            "\u001b[K     |████████████████████████████████| 550.1 MB 23 kB/s \n",
            "\u001b[K     |████████████████████████████████| 550.2 MB 29 kB/s \n",
            "\u001b[K     |████████████████████████████████| 550.1 MB 28 kB/s \n",
            "\u001b[K     |████████████████████████████████| 550.0 MB 12 kB/s \n",
            "\u001b[K     |████████████████████████████████| 549.0 MB 268 bytes/s \n",
            "\u001b[K     |████████████████████████████████| 548.9 MB 28 kB/s \n",
            "\u001b[K     |████████████████████████████████| 548.8 MB 2.2 kB/s \n",
            "\u001b[K     |████████████████████████████████| 544.6 MB 9.9 kB/s \n",
            "\u001b[K     |████████████████████████████████| 542.7 MB 5.9 kB/s \n",
            "\u001b[K     |████████████████████████████████| 542.7 MB 22 kB/s \n",
            "\u001b[K     |████████████████████████████████| 542.6 MB 27 kB/s \n",
            "\u001b[K     |████████████████████████████████| 542.7 MB 2.1 kB/s \n",
            "\u001b[K     |████████████████████████████████| 543.0 MB 23 kB/s \n",
            "\u001b[K     |████████████████████████████████| 542.8 MB 25 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 64.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 45.4 MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 2.0.7 which is incompatible.\n",
            "spacy 3.4.3 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "pymc 4.1.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "prophet 1.1.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.37.1 which is incompatible.\n",
            "google-cloud-bigquery 3.3.6 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.37.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated-nightly\n",
        "!pip install --quiet --upgrade tensorflow-model-optimization\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "EJgAOld5nLPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "8mBwlk5cnMI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "from tensorflow_model_optimization.python.core.internal import tensor_encoding as te"
      ],
      "metadata": {
        "id": "t1LwvCIBWAlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tempfile\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "J8MiURw-WGnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load simulation data.\n",
        "source, _ = tff.simulation.datasets.emnist.load_data()\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).map(\n",
        "      lambda e: (tf.reshape(e['pixels'], [-1]), e['label'])\n",
        "  ).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [client_data(n) for n in range(3)]\n",
        "\n",
        "# Wrap a Keras model for use with TFF.\n",
        "def model_fn() -> tff.learning.Model:\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),\n",
        "                            kernel_initializer='zeros')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      model,\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "  model_fn,\n",
        "  client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  state, metrics = trainer.next(state, train_data)\n",
        "  print(metrics['train']['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W3K0dnlWQUh",
        "outputId": "546c8720-b78f-4e9c-e24f-08337151a059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading emnist_all.sqlite.lzma: 100%|██████████| 170507172/170507172 [00:45<00:00, 4050252.54it/s]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.897555\n",
            "14.327109\n",
            "14.385421\n",
            "14.352639\n",
            "14.375886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCd6tShviu8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below we define several functions that we'll use later, but their details\n",
        "# aren't as important as their usage in the next cell.\n",
        "def download_movielens_data(dataset_path):\n",
        "  r = requests.get(dataset_path)\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall(path='/tmp')\n",
        "\n",
        "def load_movielens_data(data_directory=\"/tmp\"):\n",
        "  \"\"\"Loads MovieLens ratings from data directory.\"\"\"\n",
        "  ratings_df = pd.read_csv(\n",
        "      os.path.join(data_directory, \"ml-1m\", \"ratings.dat\"),\n",
        "      sep=\"::\",\n",
        "      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine=\"python\")\n",
        "  # Map movie and user IDs to [0, vocab_size).\n",
        "  movie_mapping = {\n",
        "      old_movie: new_movie for new_movie, old_movie in enumerate(\n",
        "          ratings_df.MovieID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "  user_mapping = {\n",
        "      old_user: new_user for new_user, old_user in enumerate(\n",
        "          ratings_df.UserID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "  ratings_df.MovieID = ratings_df.MovieID.map(movie_mapping)\n",
        "  ratings_df.UserID = ratings_df.UserID.map(user_mapping)\n",
        "  return ratings_df\n",
        "\n",
        "def create_tf_datasets(ratings_df,\n",
        "                       batch_size=5,\n",
        "                       max_examples_per_user=300,\n",
        "                       max_clients=2000,\n",
        "                       train_fraction=0.8):\n",
        "  \"\"\"Creates train and test TF Datasets containing the ratings for all users.\"\"\"\n",
        "  num_users = len(set(ratings_df.UserID))\n",
        "  # Limit to `max_clients` to speed up data loading.\n",
        "  num_users = min(num_users, max_clients)\n",
        "\n",
        "  def rating_batch_map_fn(rating_batch):\n",
        "    return collections.OrderedDict([\n",
        "        (\"x\", tf.cast(rating_batch[:, 1:2], tf.int64)),\n",
        "        (\"y\", tf.cast(rating_batch[:, 2:3], tf.float32))\n",
        "    ])\n",
        "\n",
        "  tf_datasets = []\n",
        "  for user_id in range(num_users):\n",
        "    user_ratings_df = ratings_df[ratings_df.UserID == user_id]\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices(user_ratings_df)\n",
        "\n",
        "    # Define preprocessing operations.\n",
        "    tf_dataset = tf_dataset.take(max_examples_per_user).shuffle(\n",
        "        buffer_size=max_examples_per_user, seed=42).batch(batch_size).map(\n",
        "        rating_batch_map_fn,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    tf_datasets.append(tf_dataset)\n",
        "\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(tf_datasets)\n",
        "  train_idx = int(len(tf_datasets) * train_fraction)\n",
        "  return (tf_datasets[:train_idx], tf_datasets[train_idx:])\n",
        "  \n",
        "\n",
        "class UserEmbedding(tf.keras.layers.Layer):\n",
        "  \"\"\"Keras layer representing an embedding for a single user, used below.\"\"\"\n",
        "\n",
        "  def __init__(self, num_latent_factors, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_latent_factors = num_latent_factors\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = self.add_weight(\n",
        "        shape=(1, self.num_latent_factors),\n",
        "        initializer='uniform',\n",
        "        dtype=tf.float32,\n",
        "        name='UserEmbeddingKernel')\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.embedding\n",
        "\n",
        "  def compute_output_shape(self):\n",
        "    return (1, self.num_latent_factors)\n",
        "\n",
        "def get_matrix_factorization_model(\n",
        "    num_items: int = 3706,\n",
        "    num_latent_factors: int = 50) -> tff.learning.reconstruction.Model:\n",
        "  \"\"\"Defines a Keras matrix factorization model.\"\"\"\n",
        "  # Layers with variables will be partitioned into global and local layers.\n",
        "  # We'll pass this to `tff.learning.reconstruction.from_keras_model`.\n",
        "  global_layers = []\n",
        "  local_layers = []\n",
        "\n",
        "  item_input = tf.keras.layers.Input(shape=[1], name='Item')\n",
        "  item_embedding_layer = tf.keras.layers.Embedding(\n",
        "      num_items,\n",
        "      num_latent_factors,\n",
        "      name='ItemEmbedding')\n",
        "  global_layers.append(item_embedding_layer)\n",
        "  flat_item_vec = tf.keras.layers.Flatten(name='FlattenItems')(\n",
        "      item_embedding_layer(item_input))\n",
        "\n",
        "  user_embedding_layer = UserEmbedding(\n",
        "      num_latent_factors,\n",
        "      name='UserEmbedding')\n",
        "  local_layers.append(user_embedding_layer)\n",
        "\n",
        "  # The item_input never gets used by the user embedding layer,\n",
        "  # but this allows the model to directly use the user embedding.\n",
        "  flat_user_vec = user_embedding_layer(item_input)\n",
        "\n",
        "  pred = tf.keras.layers.Dot(\n",
        "      1, normalize=False, name='Dot')([flat_user_vec, flat_item_vec])\n",
        "\n",
        "  input_spec = collections.OrderedDict(\n",
        "      x=tf.TensorSpec(shape=[None, 1], dtype=tf.int64),\n",
        "      y=tf.TensorSpec(shape=[None, 1], dtype=tf.float32))\n",
        "\n",
        "  model = tf.keras.Model(inputs=item_input, outputs=pred)\n",
        "  return tff.learning.reconstruction.from_keras_model(\n",
        "      keras_model=model,\n",
        "      global_layers=global_layers,\n",
        "      local_layers=local_layers,\n",
        "      input_spec=input_spec)\n",
        "  \n",
        "class RatingAccuracy(tf.keras.metrics.Mean):\n",
        "  \"\"\"Keras metric computing accuracy of reconstructed ratings.\"\"\"\n",
        "\n",
        "  def __init__(self, name='rating_accuracy', **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    absolute_diffs = tf.abs(y_true - y_pred)\n",
        "    example_accuracies = tf.less_equal(absolute_diffs, 0.5)\n",
        "    super().update_state(example_accuracies, sample_weight=sample_weight)"
      ],
      "metadata": {
        "id": "i5Lo5G8livKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_movielens_data('http://files.grouplens.org/datasets/movielens/ml-1m.zip')\n",
        "ratings_df = load_movielens_data()\n",
        "print(ratings_df.head())\n",
        "tf_train_datasets, tf_test_datasets = create_tf_datasets(ratings_df)\n",
        "\n",
        "model_fn = get_matrix_factorization_model\n",
        "loss_fn = lambda: tf.keras.losses.MeanSquaredError()\n",
        "metrics_fn = lambda: [RatingAccuracy()]\n",
        "\n",
        "training_process = tff.learning.reconstruction.build_training_process(\n",
        "    model_fn=model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0),\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.5),\n",
        "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "\n",
        "evaluation_computation = tff.learning.reconstruction.build_federated_evaluation(\n",
        "    model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "\n",
        "state = training_process.initialize()\n",
        "for i in range(10):\n",
        "  federated_train_data = np.random.choice(tf_train_datasets, size=50, replace=False).tolist()\n",
        "  state, metrics = training_process.next(state, federated_train_data)\n",
        "  print(f'Train round {i}:', metrics['train'])\n",
        "\n",
        "eval_metrics = evaluation_computation(state.model, tf_test_datasets)\n",
        "print('Final Eval:', eval_metrics['eval'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b79sT8RCixlx",
        "outputId": "6ed90308-d020-40e3-b9d1-86a150ae2d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   UserID  MovieID  Rating  Timestamp\n",
            "0       0     1104       5  978300760\n",
            "1       0      639       3  978302109\n",
            "2       0      853       3  978301968\n",
            "3       0     3177       4  978300275\n",
            "4       0     2162       5  978824291\n",
            "Train round 0: OrderedDict([('rating_accuracy', 0.0), ('loss', 14.142631)])\n",
            "Train round 1: OrderedDict([('rating_accuracy', 0.0), ('loss', 14.824729)])\n",
            "Train round 2: OrderedDict([('rating_accuracy', 0.0), ('loss', 13.9929)])\n",
            "Train round 3: OrderedDict([('rating_accuracy', 0.005263158), ('loss', 12.042918)])\n",
            "Train round 4: OrderedDict([('rating_accuracy', 0.04540295), ('loss', 8.871599)])\n",
            "Train round 5: OrderedDict([('rating_accuracy', 0.111754686), ('loss', 5.9826117)])\n",
            "Train round 6: OrderedDict([('rating_accuracy', 0.13494694), ('loss', 5.4368486)])\n",
            "Train round 7: OrderedDict([('rating_accuracy', 0.20398773), ('loss', 3.5520089)])\n",
            "Train round 8: OrderedDict([('rating_accuracy', 0.18127786), ('loss', 4.023481)])\n",
            "Train round 9: OrderedDict([('rating_accuracy', 0.24342296), ('loss', 3.189603)])\n",
            "Final Eval: OrderedDict([('loss', 2.9541085), ('rating_accuracy', 0.24831593)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load simulation data.\n",
        "source, _ = tff.simulation.datasets.emnist.load_data()\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).map(\n",
        "      lambda e: (tf.reshape(e['pixels'], [-1]), e['label'])\n",
        "  ).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [client_data(n) for n in range(3)]\n",
        "\n",
        "# Wrap a Keras model for use with TFF.\n",
        "def model_fn() -> tff.learning.Model:\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),\n",
        "                            kernel_initializer='zeros')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      model,\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "  \n",
        "# Construct DP model update aggregator.\n",
        "model_update_aggregator = tff.learning.dp_aggregator(\n",
        "  noise_multiplier=1e-3,   # z: Determines privacy epsilon.\n",
        "  clients_per_round=3)     # Aggregator needs number of clients per round.\n",
        "\n",
        "# Build FedAvg process with custom aggregator.\n",
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "  model_fn,\n",
        "  client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n",
        "  model_update_aggregation_factory=model_update_aggregator)\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  state, metrics = trainer.next(state, train_data)\n",
        "  print(metrics['train']['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bz2SCPUlXIK",
        "outputId": "458c73fd-4a97-4b5d-c144-18ea69f6584d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.897555\n",
            "13.928604\n",
            "14.069529\n",
            "14.033853\n",
            "13.983997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load simulation data.\n",
        "source, _ = tff.simulation.datasets.emnist.load_data()\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).map(\n",
        "      lambda e: (tf.reshape(e['pixels'], [-1]), e['label'])\n",
        "  ).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [client_data(n) for n in range(3)]\n",
        "\n",
        "# Wrap a Keras model for use with TFF.\n",
        "def model_fn() -> tff.learning.Model:\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),\n",
        "                            kernel_initializer='zeros')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      model,\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "  \n",
        "# Construct Tree Aggregation aggregator for DP-FTRL.\n",
        "model_weight_specs = tff.framework.type_to_tf_tensor_specs(\n",
        "        tff.learning.framework.weights_type_from_model(model_fn).trainable)\n",
        "model_update_aggregator = tff.aggregators.DifferentiallyPrivateFactory.tree_aggregation(\n",
        "    noise_multiplier=1e-3,\n",
        "    clients_per_round=3,\n",
        "    l2_norm_clip=1.,\n",
        "    record_specs=model_weight_specs)  \n",
        "\n",
        "# Build FedAvg process with custom aggregator.\n",
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "  model_fn,\n",
        "  client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n",
        "  model_update_aggregation_factory=model_update_aggregator)\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  state, metrics = trainer.next(state, train_data)\n",
        "  print(metrics['train']['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75rwIPGHlj24",
        "outputId": "07721968-1140-439e-f30c-b49a469e0671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.897555\n",
            "14.187862\n",
            "14.361877\n",
            "13.692983\n",
            "14.062213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the simulation data.\n",
        "source, _ = tff.simulation.datasets.shakespeare.load_data()\n",
        "\n",
        "# Preprocessing funtion to tokenize a line into words.\n",
        "@tf.function\n",
        "def tokenize(ds):\n",
        "  \"\"\"Tokenizes a line into words with alphanum characters.\"\"\"\n",
        "  def extract_strings(example):\n",
        "    return tf.expand_dims(example['snippets'], 0)\n",
        "\n",
        "  def tokenize_line(line):\n",
        "    return tf.data.Dataset.from_tensor_slices(tokenizer.tokenize(line)[0])\n",
        "\n",
        "  def mask_all_symbolic_words(word):\n",
        "    return tf.math.logical_not(\n",
        "        tf_text.wordshape(word, tf_text.WordShape.IS_PUNCT_OR_SYMBOL))\n",
        "\n",
        "  tokenizer = tf_text.WhitespaceTokenizer()\n",
        "  ds = ds.map(extract_strings)\n",
        "  ds = ds.flat_map(tokenize_line)\n",
        "  ds = ds.map(tf_text.case_fold_utf8)\n",
        "  ds = ds.filter(mask_all_symbolic_words)\n",
        "  return ds\n",
        "\n",
        "# Arguments for the PHH computation\n",
        "batch_size = 5\n",
        "max_words_per_user = 8\n",
        "\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return tokenize(source.create_tf_dataset_for_client(\n",
        "      source.client_ids[n])).batch(batch_size)\n",
        "\n",
        "# Pick a subset of client devices to participate in the PHH computation.\n",
        "dataset = [client_data(n) for n in range(10)]\n",
        "\n",
        "def run_simulation(one_round_computation: tff.Computation, dataset):\n",
        "  output = one_round_computation(dataset)\n",
        "  heavy_hitters = output.heavy_hitters\n",
        "  heavy_hitters_counts = output.heavy_hitters_counts\n",
        "  heavy_hitters = [word.decode('utf-8', 'ignore') for word in heavy_hitters]\n",
        "\n",
        "  results = {}\n",
        "  for index in range(len(heavy_hitters)):\n",
        "    results[heavy_hitters[index]] = heavy_hitters_counts[index]\n",
        "  return dict(results)\n",
        "\n",
        "iblt_computation = tff.analytics.heavy_hitters.iblt.build_iblt_computation(\n",
        "    capacity=100,\n",
        "    max_string_length=20,\n",
        "    max_words_per_user=max_words_per_user,\n",
        "    max_heavy_hitters=10,\n",
        "    multi_contribution=False,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "result = run_simulation(iblt_computation, dataset)\n",
        "print(f'result without DP: {result}')\n",
        "\n",
        "# DP parameters\n",
        "eps = 20\n",
        "delta = 0.01\n",
        "\n",
        "# Calculating scale for Laplace noise\n",
        "scale = max_words_per_user / eps\n",
        "\n",
        "# Calculating the threshold\n",
        "tau = 1 + (max_words_per_user / eps) * np.log(max_words_per_user / (2 * delta))\n",
        "\n",
        "result_with_dp = {}\n",
        "for word in result:\n",
        "  noised_count = result[word] + np.random.laplace(scale=scale)\n",
        "  if noised_count >= tau:\n",
        "    result_with_dp[word] = noised_count\n",
        "print(f'result with DP: {result_with_dp}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "vkjY75_qlycG",
        "outputId": "8edca76e-2052-4493-f7aa-283494726436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-aa0bfa2fff1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the simulation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshakespeare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Preprocessing funtion to tokenize a line into words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tff' is not defined"
          ]
        }
      ]
    }
  ]
}